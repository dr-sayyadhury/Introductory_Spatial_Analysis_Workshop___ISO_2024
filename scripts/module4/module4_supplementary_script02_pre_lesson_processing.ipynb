{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> **ISO2024 INTRODUCTORY SPATIAL 'OMICS ANALYSIS**\n",
    ">\n",
    ">\n",
    ">- HYBRID : TORONTO & ZOOM\n",
    ">- 10TH JULY 2024 <br>\n",
    "\n",
    "\n",
    ">**Module 4 PRE-WORK - Supplementary file 1 <br>\n",
    ">**Instructor : Shamini Ayyadhury**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Supplementary script 02 : PREPARING THE FOLLOWING FILES FOR SEGMENTATION AND SEG-FREE ANALYSIS\n",
    "\n",
    "    1. CROPPED IMAGE OF ALL 4 CHANNELS \n",
    "    2. COMPOSITE IMAGE OF ALL 4 CHANNELS COMBINED\n",
    "    3. SUBSETTED TRANSCRIPTS FILE MATCHING THE CROPPED IMAGE DIMENSIONS\n",
    "    4. SUBSETTED CELL AND NUCLEAR BOUNDARIES FILES MATCHING THE CROPPED IMAGE DIMENSIONS\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import the following libraries\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scanpy as sc\n",
    "#from PIL import Image\n",
    "import os\n",
    "import warnings\n",
    "import tifffile as tiff\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "sys.path.append('/home/shamini/data/projects/spatial_workshop/')\n",
    "import pre_processing_fnc as ppf # import memory usage function only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### directory & filepaths\n",
    "data_dir = '/home/shamini/data1/data_orig/data/spatial/xenium/10xGenomics/cell_seg_brain_cancer/'\n",
    "out = '/home/shamini/data/projects/spatial_workshop/out/module4/'\n",
    "os.makedirs(out, exist_ok=True) # create a new directory to store the output files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts = 'transcripts.parquet'\n",
    "image = 'morphology_focus_0000.ome.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transcript = pd.read_parquet(data_dir+'out/'+transcripts)\n",
    "iF = tiff.imread(data_dir+'out/morphology_focus/'+image)\n",
    "\n",
    "#----------------------------------------------\n",
    "ppf.get_memory_usage() ### monitor memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scale factor is required to convert the metric units of the transcript coordinates into pixel units to ensure that we ca superimpose the image, transcripts and polygon information precisely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 0.2125 ### size of each pixel in microns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to convert the um scale to a pixel scale, we need to divide each transcript coordinate unit (um) by the pixel size (um/pixel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transcript[['x_location']] = df_transcript[['x_location']]/scale\n",
    "df_transcript[['y_location']] = df_transcript[['y_location']]/scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crop and subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "I have arbitrarily chosen the following coordinates to crop the image. The reason for this choice was \n",
    "1. To have a smaller image to work with during the tutorials\n",
    "2. To select regions that had different cell-types including blood vessels.\n",
    "'''\n",
    "\n",
    "xmin_cut = int(10000/scale)\n",
    "xmax_cut = int(11000/scale)\n",
    "\n",
    "ymin_cut = int(6000/scale)\n",
    "ymax_cut = int(7000/scale)\n",
    "\n",
    "print(xmin_cut, xmax_cut, ymin_cut, ymax_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset the transcripts file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transcripts_subset = df_transcript[((df_transcript['x_location'] > xmin_cut) & (df_transcript['x_location'] < xmax_cut)) & ((df_transcript['y_location'] > ymin_cut) & (df_transcript['y_location'] < ymax_cut))]\n",
    "sns.scatterplot(data=transcripts_subset, x='x_location', y='y_location', s=.1)\n",
    "\n",
    "\n",
    "#----------------------------------------------\n",
    "ppf.get_memory_usage() ### monitor memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WE need to tag the file as we did in module 2, script 01. But we only remove the neg probe transcripts. We keep the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "processed_tf = ppf.process_data(transcripts_subset)\n",
    "processed_tf = processed_tf[processed_tf['group']=='gene_probes']\n",
    "\n",
    "#----------------------------------------------\n",
    "ppf.get_memory_usage() ### monitor memory usage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we subset the transcript file, we will find the minimum and maximum y and x pixel coordinates, convert them to intergers, before using these to offset the coordinates to fit the cropped image coordinates.\n",
    "\n",
    "Note : why is this step important? \n",
    "1. Because the cropped image will automatically revert to a (0,0) origin and to ensure effective image to transcript registration, the coordinate system has to be reset. \n",
    "2. It is important to use an integer value as while the transcript spatial coordinate framework is on a continous scale, the image scale is discrete. (the image will read an error when you try to offset it by a float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### has tO be an integer as images are not on a continous scale transcript coordinates are\n",
    "\n",
    "min_x = int(processed_tf['x_location'].min())\n",
    "max_x = int(processed_tf['x_location'].max())\n",
    "\n",
    "min_y = int(processed_tf['y_location'].min())\n",
    "max_y = int(processed_tf['y_location'].max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy back the processed_tf back to transcript_subset, replacing the file before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transcripts_subset = processed_tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min_x, max_x, min_y, max_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reset the coordinates to ensure it matches with the cropped image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_subset = transcripts_subset[transcripts_subset['qv']>20]\n",
    "transcripts_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### reset the coordinates to 0\n",
    "transcripts_subset['x_location'] = transcripts_subset['x_location'] - min_x\n",
    "transcripts_subset['y_location'] = transcripts_subset['y_location'] - min_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### this file will be used subsquently for baysor segmentationa and seg free analysis\n",
    "transcripts_subset.to_csv(out+'/transcripts_subset_all_genes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### crop the image\n",
    "fig, ax = plt.subplots(2, 2, figsize=(16, 16))\n",
    "iF_crop = iF[:, min_y:max_y, min_x:max_x]\n",
    "ax = ax.flatten()\n",
    "\n",
    "for channel in range(4):\n",
    "    ax[channel].imshow(iF_crop[channel], cmap='gray')\n",
    "    ax[channel].axis('off')\n",
    "    ax[channel].set_title(f'Channel {channel}') \n",
    "    \n",
    "#----------------------------------------------\n",
    "ppf.get_memory_usage() ### monitor memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save the cropped image\n",
    "tiff.imsave(out+'/cropped_image_fluo.tif', iF_crop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if a common coordinate framework is maintained by plotting the transcripts over the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "transcripts_subset_sample = transcripts_subset.sample(frac=0.3, random_state=1)\n",
    "\n",
    "axes[0].imshow(iF_crop[1,:,:], cmap='magma')\n",
    "ax = sns.scatterplot(data=transcripts_subset_sample, x='x_location', y='y_location', s=0.3, ax=axes[0], alpha=0.5, color='white')\n",
    "axes[0].set_title('Cropped Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(iF_crop[1,:,:], cmap='magma')\n",
    "ax = sns.scatterplot(data=transcripts_subset_sample, x='x_location', y='y_location', s=0.9, ax=axes[1], alpha=0.75, color='white')\n",
    "ax.set_xlim(3000, 3500)\n",
    "ax.set_ylim(3000, 3500)\n",
    "\n",
    "\n",
    "#----------------------------------------------\n",
    "ppf.get_memory_usage() ### monitor memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, repeat the same for the polygons for cell and nuclear boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import cell boundaries and nuclear boundaries files\n",
    "cell_boundaries = pd.read_parquet(data_dir+'out/cell_boundaries.parquet')\n",
    "nuclear_boundaries = pd.read_parquet(data_dir+'out/nucleus_boundaries.parquet')\n",
    "\n",
    "\n",
    "#----------------------------------------------\n",
    "ppf.get_memory_usage() ### monitor memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_boundaries = cell_boundaries[cell_boundaries['cell_id'].isin(df_transcript['cell_id'])]\n",
    "cell_boundaries['vertex_x'] = (cell_boundaries['vertex_x']/0.2125)\n",
    "cell_boundaries['vertex_y'] = (cell_boundaries['vertex_y']/0.2125)\n",
    "\n",
    "nuclear_boundaries = nuclear_boundaries[nuclear_boundaries['cell_id'].isin(df_transcript['cell_id'])]\n",
    "nuclear_boundaries['vertex_x'] = (nuclear_boundaries['vertex_x']/0.2125)\n",
    "nuclear_boundaries['vertex_y'] = (nuclear_boundaries['vertex_y']/0.2125)\n",
    "\n",
    "\n",
    "#----------------------------------------------\n",
    "ppf.get_memory_usage() ### monitor memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### shift the coordinates to match the cropped image\n",
    "cell_boundaries['vertex_x'] = cell_boundaries['vertex_x'] - min_x\n",
    "cell_boundaries['vertex_y'] = cell_boundaries['vertex_y'] - min_y\n",
    "\n",
    "nuclear_boundaries['vertex_x'] = nuclear_boundaries['vertex_x'] - min_x\n",
    "nuclear_boundaries['vertex_y'] = nuclear_boundaries['vertex_y'] - min_y\n",
    "\n",
    "#----------------------------------------------\n",
    "ppf.get_memory_usage() ### monitor memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_boundaries.to_parquet(out+'cell_boundaries_subset.parquet')\n",
    "nuclear_boundaries.to_parquet(out+'nuclear_boundaries_subset.parquet')\n",
    "\n",
    "#----------------------------------------------\n",
    "ppf.get_memory_usage() ### monitor memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we provide with a matched cropped anndata object of the adata file transformed in script 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### subset adata to the cropped image\n",
    "\n",
    "adata = sc.read_h5ad(out+'adata.h5ad')\n",
    "print(f'Adata before cropping:\\n {adata}\\n')\n",
    "adata_crop = adata[adata.obs.index.isin(transcripts_subset['cell_id']),:]\n",
    "print(f'Adata after cropping:\\n {adata_crop}')\n",
    "\n",
    "\n",
    "#----------------------------------------------\n",
    "ppf.get_memory_usage() ### monitor memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the segmentation module, we will be evaluating 3 genes to better understand how the different segmetnation parameters affect transcript assignment and cell-type interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes = ['PTPRC', 'ANXA1', 'STMN1']\n",
    "\n",
    "adata_3g = adata_crop[:,genes]\n",
    "adata_3g.write_h5ad(out+'adata_3g.h5ad')   \n",
    "adata_crop.write_h5ad(out+'adata_subset.h5ad') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_subset_3g = transcripts_subset[transcripts_subset['feature_name'].isin(genes)]\n",
    "transcripts_subset_3g.to_csv(out+'transcripts_subset_3g.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_boundaries = cell_boundaries[cell_boundaries['cell_id'].isin(transcripts_subset['cell_id'])]\n",
    "nuclear_boundaries = nuclear_boundaries[nuclear_boundaries['cell_id'].isin(transcripts_subset['cell_id'])]\n",
    "\n",
    "cell_boundaries.to_csv(out+'cell_boundaries_subset.csv', index=False)\n",
    "nuclear_boundaries.to_csv(out+'nuclear_boundaries_subset.csv', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xenium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
