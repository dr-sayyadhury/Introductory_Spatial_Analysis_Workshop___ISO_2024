{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> **ISO2024 INTRODUCTORY SPATIAL 'OMICS ANALYSIS**\n",
    ">\n",
    ">\n",
    ">- HYBRID : TORONTO & ZOOM\n",
    ">- 10TH JULY 2024 <br>\n",
    "\n",
    ">**Module 6 : Drawing the boundaries ** <BR>\n",
    ">\n",
    ">**Instructor : Shamini Ayyadhury**\n",
    ">\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TOPICS COVERED\n",
    "\n",
    "* A. Classical segmentation\n",
    "* B. Segmentation-free\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "B. SEGMENTATION-FREE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. NOW WE WILL PERFORM BAYSOR NON-IMAGE BASED SEGMENTATION\n",
    "2. BAYSOR IS VERY MEMORY-INTENSIVE - SO WE WILL TEST A TOY DATASET HERE - YES A MUCH SMALLER SAMPLE SIZE THAN THE ALREADY DOWNSIZED TRANSCRIPT SAMPLE.\n",
    "3. AFTER WHICH, YOU WILL USE THE BAYSOR OUTPUT THAT WE HAVE ALREADY PROCESSED FOR YOU, FOR ALL DOWNSTREAM ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import sys\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.append('/home/shamini/data/projects/spatial_workshop/')\n",
    "import pre_processing_fnc as ppf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### directory & filepaths\n",
    "data_dir = '/home/shamini/data1/data_orig/data/spatial/xenium/10xGenomics/cell_seg_brain_cancer/'\n",
    "out = '/home/shamini/data/projects/spatial_workshop/out/module4/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> DEMO - STEP "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step has already been done for you. <br>\n",
    "See supplementary script 03\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  BAYSOR SEGMENATION STEP\n",
    "\n",
    "# get filtered transcript files\n",
    "### Read in the transcripts_df.csv file from module 2\n",
    "#os.makedirs(out+'baysor_test', exist_ok=True)\n",
    "#os.makedirs(out+'baysor_test/segmentation', exist_ok=True)\n",
    "\n",
    "#command = [\n",
    "#        '/home/shamini/baysor/bin/baysor/bin/baysor', 'run',\n",
    "#        '-c', out+'baysor/parameters_baysor.toml',\n",
    "#        '-s', '9',\n",
    "#        '--save-polygons', 'geojson',\n",
    "#        '-o', os.path.join(out, 'baysor_test/segmentation/'),\n",
    "#        os.path.join(out,'transcripts_subset_all_genes_downsampled.csv')\n",
    "#        ]\n",
    "#subprocess.run(command, check=True)\n",
    "\n",
    "#----------------------------------------------\n",
    "#ppf.get_memory_usage() ### monitor memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step has already been done for you. <br>\n",
    "See supplementary script 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NCV STEP\n",
    "\n",
    "#os.makedirs(out+'baysor_test/ncv', exist_ok=True)\n",
    "#command = [\n",
    "#        '/home/shamini/baysor/bin/baysor/bin/baysor', 'segfree',\n",
    "#        '-c', out+'baysor/parameters_baysor.toml',\n",
    "#        '-o', os.path.join(out, 'baysor_test/ncv'),\n",
    "#        os.path.join(data_dir,'module4/transcripts_subset_all_genes_downsampled.csv')\n",
    "#        ]\n",
    "#subprocess.run(command, check=True)\n",
    "\n",
    "#----------------------------------------------\n",
    "#ppf.get_memory_usage() ### monitor memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<<< TOY RUN END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> Now we will read in the previously processed baysor segmentation files, which is matched with the image we analyzed in our previous script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### load baysor output\n",
    "baysor_seg = pd.read_csv(out+'baysor/segmentation/scale9/segmentation.csv')\n",
    "\n",
    "\n",
    "#----------------------------------------------\n",
    "ppf.get_memory_usage() ### monitor memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> DISCUSSION : THE OUTPUT TABLE BELOW SHOWS SOME NEW ADDITIONS AS WELL AS SOME FAMILAR COLUMNS FROM THE ORIGINAL TRANSCRIPT FILE . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baysor_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRANSCRIPT ASSIGNMENT \n",
    "\n",
    "print('Baysor transcript assignment:\\n ', baysor_seg['is_noise'].value_counts(normalize=True)), \n",
    "print('\\n Xenium transcript assignment:\\n', baysor_seg['binary'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WE WILL NOW CLEAN THE BAYSOR SEGMENTATION OUTPUT TO REMOVE UNASSINGED TRANSCRIPTS AND IMPORT THE SEGMENTATION POLYGON JSON FILE FROM THE BAYSOR RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### drop the noise cells\n",
    "baysor_seg_clean = baysor_seg[baysor_seg['is_noise'] == False]\n",
    "\n",
    "### get rid of cells with low assignment confidence\n",
    "baysor_seg_clean = baysor_seg_clean[baysor_seg_clean['assignment_confidence'] > 0.7]\n",
    "\n",
    "### old and new segmentation files\n",
    "\n",
    "import json\n",
    "\n",
    "with open(out+'baysor/segmentation/scale9/segmentation_polygons.json') as f:\n",
    "    baysor_boundaries = json.load(f)\n",
    "\n",
    "orig_boundaries = pd.read_csv(out+'cell_boundaries_subset.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNLIKE THE ORIGINAL XENIUM SEGMENTATION POLYGON FILE, WHICH WAS A DATAFRAME, THE BAYSOR SEGMENTATION POLYGONS ARE STORES AS A JSON FILE. <BR>\n",
    "SINCE WE REMOVED TRANSCIPTS THAT WERE UNASSIGNED, WE NEED TO ENSURE WE REMOVE ANY EMPTY CELLS AS A RESULT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Extract cell IDs from the DataFrame after converting them to the correct format\n",
    "matching_cell_ids = baysor_seg_clean['cell'].str.split('-').str[1].astype(int).unique()\n",
    "\n",
    "# Filter the JSON data to include only the relevant cell IDs\n",
    "baysor_boundaries_filtered = [geom for geom in baysor_boundaries['geometries'] if geom['cell'] in matching_cell_ids]\n",
    "\n",
    "# Verify the number of filtered geometries\n",
    "print(\"Number of filtered geometries:\", len(baysor_boundaries_filtered))\n",
    "\n",
    "# Create the new JSON structure\n",
    "filtered_json_data = {'geometries': baysor_boundaries_filtered}\n",
    "\n",
    "# Convert the filtered data to a JSON string for easier inspection\n",
    "filtered_json_str = json.dumps(filtered_json_data, indent=4)\n",
    "\n",
    "# Print the filtered JSON string\n",
    "print(filtered_json_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW, LET'S PLOT AND COMPARE THE IMAGE-BASED AND NON-IMAGE-BASED METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plotting the segmentation results\n",
    "### Note that you have been given baysor segmentation carried out on 3 different scales. Plot each and \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "import tifffile as tiff\n",
    "\n",
    "\n",
    "iF_crop = tiff.imread(data_dir+'module4/cropped_image_fluo.tif')\n",
    "composite_image = ppf.plot_composite_image(iF_crop)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 10))\n",
    "\n",
    "xlower = 0\n",
    "ylower = 2000\n",
    "xlim = [xlower, xlower+1200]\n",
    "ylim = [ylower, ylower+1200]\n",
    "\n",
    "### plot the original segmentation\n",
    "\n",
    "ax[0].imshow(composite_image)\n",
    "\n",
    "grouped = orig_boundaries.groupby('cell_id')\n",
    "\n",
    "for cell_id, group in grouped:\n",
    "    group = pd.concat([group, group.iloc[:1]])\n",
    "    plg = Polygon(group[['vertex_x', 'vertex_y']].values, edgecolor='red', facecolor='none')\n",
    "    ax[0].add_patch(plg)\n",
    "\n",
    "ax[0].set_xlim(xlim)\n",
    "ax[0].set_ylim(ylim)\n",
    "ax[0].set_title('Original segmentation')\n",
    "\n",
    "\n",
    "### now plot the baysor segmentation which is a json file\n",
    "ax[1].imshow(composite_image)\n",
    "\n",
    "\n",
    "for geom in filtered_json_data['geometries']:\n",
    "    plg = geom['coordinates'][0]\n",
    "    plg.append(plg[0])\n",
    "    polygon = plt.Polygon(plg, edgecolor='red', facecolor='none', closed=True)\n",
    "    ax[1].add_patch(polygon)\n",
    "    ax[1].set_xlim(xlim)\n",
    "    ax[1].set_ylim(ylim)\n",
    "ax[1].set_title('Baysor segmentation')\n",
    "   \n",
    "ax[1].set_xlim(xlim)\n",
    "ax[1].set_ylim(ylim)\n",
    "ax[1].set_title('Baysor segmentation')\n",
    "\n",
    "     \n",
    "\n",
    "#----------------------------------------------\n",
    "ppf.get_memory_usage() ### monitor memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> Note that the above baysor segmentation is oversegmented. <br>\n",
    ">>> If you plot the next scale done, scale=45, it will be under-segmented.<br>\n",
    "\n",
    ">>> Try repeating the baysor algorithm on your own, using various scaled between 9 and 45."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW LET'S EVALUATE THE TRANSCRIPT COMPOSITION AND OVERLAP OF THE FOLLOWING GENES : ANXA1, PTPRC AND STMN1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of genes of interest\n",
    "genes = ['PTPRC', 'ANXA1', 'STMN1']\n",
    "\n",
    "# Subset the data\n",
    "baysor_seg_genes = baysor_seg_clean[baysor_seg_clean['gene'].isin(genes)]\n",
    "\n",
    "# Function to get unique cells expressing specific genes\n",
    "def get_unique_cells(data, gene, cell_col='cell_id'):\n",
    "    return data[cell_col][data['gene'] == gene].unique()\n",
    "\n",
    "# Original/Xenium data\n",
    "cells_stmn1 = get_unique_cells(baysor_seg_genes, 'STMN1')\n",
    "cells_anxa1 = get_unique_cells(baysor_seg_genes, 'ANXA1')\n",
    "cells_ptprc = get_unique_cells(baysor_seg_genes, 'PTPRC')\n",
    "\n",
    "# Total cells in original/Xenium\n",
    "total_cells_orig = len(set(cells_stmn1) | set(cells_anxa1) | set(cells_ptprc))\n",
    "\n",
    "# Cells expressing combinations of genes (original/Xenium)\n",
    "cells_all = set(cells_stmn1) & set(cells_anxa1) & set(cells_ptprc)\n",
    "cells_stmn1_anxa1 = set(cells_stmn1) & set(cells_anxa1)\n",
    "cells_stmn1_ptprc = set(cells_stmn1) & set(cells_ptprc)\n",
    "cells_anxa1_ptprc = set(cells_anxa1) & set(cells_ptprc)\n",
    "\n",
    "# Baysor data\n",
    "cells_stmn1_b = get_unique_cells(baysor_seg_genes, 'STMN1', cell_col='cell')\n",
    "cells_anxa1_b = get_unique_cells(baysor_seg_genes, 'ANXA1', cell_col='cell')\n",
    "cells_ptprc_b = get_unique_cells(baysor_seg_genes, 'PTPRC', cell_col='cell')\n",
    "\n",
    "# Total cells in Baysor\n",
    "total_cells_baysor = len(set(cells_stmn1_b) | set(cells_anxa1_b) | set(cells_ptprc_b))\n",
    "\n",
    "# Cells expressing combinations of genes (Baysor)\n",
    "cells_all_b = set(cells_stmn1_b) & set(cells_anxa1_b) & set(cells_ptprc_b)\n",
    "cells_stmn1_anxa1_b = set(cells_stmn1_b) & set(cells_anxa1_b)\n",
    "cells_stmn1_ptprc_b = set(cells_stmn1_b) & set(cells_ptprc_b)\n",
    "cells_anxa1_ptprc_b = set(cells_anxa1_b) & set(cells_ptprc_b)\n",
    "\n",
    "# Create a DataFrame to store the normalized counts\n",
    "comparison_table = pd.DataFrame({\n",
    "    'Condition': [\n",
    "        'STMN1', 'ANXA1', 'PTPRC',\n",
    "        'STMN1 & ANXA1', 'STMN1 & PTPRC', 'ANXA1 & PTPRC',\n",
    "        'All 3 Genes'\n",
    "    ],\n",
    "    'Original/Xenium': [\n",
    "        len(cells_stmn1) / total_cells_orig, len(cells_anxa1) / total_cells_orig, len(cells_ptprc) / total_cells_orig,\n",
    "        len(cells_stmn1_anxa1) / total_cells_orig, len(cells_stmn1_ptprc) / total_cells_orig, len(cells_anxa1_ptprc) / total_cells_orig,\n",
    "        len(cells_all) / total_cells_orig\n",
    "    ],\n",
    "    'Baysor': [\n",
    "        len(cells_stmn1_b) / total_cells_baysor, len(cells_anxa1_b) / total_cells_baysor, len(cells_ptprc_b) / total_cells_baysor,\n",
    "        len(cells_stmn1_anxa1_b) / total_cells_baysor, len(cells_stmn1_ptprc_b) / total_cells_baysor, len(cells_anxa1_ptprc_b) / total_cells_baysor,\n",
    "        len(cells_all_b) / total_cells_baysor\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Display the comparison table\n",
    "print(comparison_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Always remember that is important to evaluate both the images and the above proportions when evaluating segmentation algorithms\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> DISCUSSION\n",
    "1. WHAT HAVE WE LEARNT FROM THIS TUTORIAL? \n",
    "2. WHICH SEGMENTATION METHOD DO YOU PREFER?\n",
    "3. DOES THIS MEAN MACHINE LEARNING IMAGE-BASED MODELS ARE BETTER AT SEGMENTATION?\n",
    "4. WHAT USE DO WE HAVE THEN FOR STATISTICALLY GUIDED TOOLS SUCH AS BAYSOR?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> SEG-FREE ANALYSIS\n",
    "\n",
    ">>> STOP TO EXPLAIN CAREFULLY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Baysor also provides a ncv output file.\n",
    "2. This is the 'neighborhood compositional vector' based on single molecule clustering (transcript clustering and the gene compositions of local clusters)\n",
    "\n",
    "```\n",
    "IMPORTANT\n",
    "```\n",
    "NCV compute time is long.\n",
    "The loompy package is not installed.\n",
    "1. We have extracted the relevant NCV values and saved them.\n",
    "2. You will be using these pre-computed values and plotting these\n",
    "\n",
    "The following code has already been run for you. \n",
    "\n",
    "---\n",
    "import loompy <br>\n",
    "\n",
    "### read loom file \n",
    "lm = loompy.connect(out+'baysor/ncv/ncvs.loom') <br>\n",
    "\n",
    "lm_ptprc = lm[lm.ra['Name']=='PTPRC',:].T <br>\n",
    "lm_stmn1 = lm[lm.ra['Name']=='STMN1',:].T <br>\n",
    "lm_anxa1 = lm[lm.ra['Name']=='ANXA1',:].T <br>\n",
    "\n",
    "baysor_seg['ncv_ptprc'] = lm_ptprc <br>\n",
    "baysor_seg['ncv_stmn1'] = lm_stmn1 <br>\n",
    "baysor_seg['ncv_anxa1'] = lm_anxa1 <br>\n",
    "\n",
    "lm.close() <br>\n",
    "\n",
    "baysor_seg.to_csv(out+'baysor/ncv/baysor_seg_ncv.csv', index=False) <br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Read the pre-computed baysor segmentation file that aready has the ncv vectors for PTPRC, ANXA1 and STMN1 appended\n",
    "* We will drop the low confidence cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Look at the baysor_seg file that has the ncv details for PTPC, ANXA1 qnd STMN1 appended\n",
    "\n",
    "baysor_seg = pd.read_csv(out+'baysor/ncv/baysor_seg_ncv.csv')\n",
    "baysor_seg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### drop the noise cells\n",
    "baysor_seg_clean = baysor_seg[baysor_seg['is_noise'] == False]\n",
    "\n",
    "### get rid of cells with low assignment confidence\n",
    "baysor_seg_clean = baysor_seg_clean[baysor_seg_clean['assignment_confidence'] > 0.7]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baysor_seg_downsampled = baysor_seg_clean[(baysor_seg_clean['x'] < 600) & (baysor_seg_clean['y'] < 600)].copy()\n",
    "baysor_seg_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib import patches as mpatches\n",
    "\n",
    "### to save on compute time, we will plot only a fraction of the transcripts \n",
    "frac = 0.25\n",
    "\n",
    "### subset only ncv values for the genes of interest, greater than zero\n",
    "baysor_seg_downsampled_ptprc = baysor_seg_downsampled[baysor_seg_downsampled['ncv_ptprc'] > 0]\n",
    "baysor_seg_downsampled_stmn1 = baysor_seg_downsampled[baysor_seg_downsampled['ncv_stmn1'] > 0]\n",
    "baysor_seg_downsampled_anxa1 = baysor_seg_downsampled[baysor_seg_downsampled['ncv_anxa1'] > 0]\n",
    "\n",
    "### define the subplots \n",
    "fig, axes = plt.subplots(2, 2, figsize=(9, 7))\n",
    "axes = axes.flatten()\n",
    "\n",
    "### first we group the values by ncv color - recall lecture\n",
    "grouped = baysor_seg_downsampled.groupby('ncv_color')\n",
    "\n",
    "### PLOT 1 - here will plot all transcripts by their pre-calculated NCV color values\n",
    "for name, group in grouped:\n",
    "    group = group.sample(frac=frac)\n",
    "    sns.scatterplot(group, x='x', y='y', color= group['ncv_color'].unique(), ax=axes[0], s=1, rasterized=True)\n",
    "sns.despine()\n",
    "axes[0].set_title('All transcripts by NCV color')\n",
    "\n",
    "### PLOT 2 - on the adjacent plot, we will the plot the gene expression distribution for the three genes of interest\n",
    "ax = sns.scatterplot(data=baysor_seg_downsampled_ptprc, x='x', y='y', hue='ncv_ptprc', s=1, ax=axes[1], palette='Greens')\n",
    "ax.legend().remove()\n",
    "ax = sns.scatterplot(data=baysor_seg_downsampled_stmn1, x='x', y='y', hue='ncv_stmn1', s=1, ax=axes[1], palette='Reds')\n",
    "ax.legend().remove()\n",
    "ax = sns.scatterplot(data=baysor_seg_downsampled_anxa1, x='x', y='y', hue='ncv_anxa1', s=1, ax=axes[1], palette='Blues')\n",
    "ax.legend().remove()\n",
    "ax.set_title('Gene expression distribution')\n",
    "\n",
    "# PLOT 3 - Same as plot 1 but a zoomed-in section\n",
    "for name, group in grouped:\n",
    "    group = group.sample(frac=frac)\n",
    "    sns.scatterplot(group, x='x', y='y', color= group['ncv_color'].unique(), ax=axes[2], s=9, rasterized=True)\n",
    "sns.despine()\n",
    "axes[2].set_xlim(200, 500)\n",
    "axes[2].set_ylim(300, 600)  \n",
    "axes[2].set_title('All transcripts by NCV color (zoomed-in)')\n",
    "\n",
    "# PLOT 4 - Same as plot 2 but a zoomed-in section\n",
    "ax = sns.scatterplot(data=baysor_seg_downsampled_ptprc, x='x', y='y', hue='ncv_ptprc', s=9, ax=axes[3], palette='Greens')\n",
    "ax.legend().remove()\n",
    "ax = sns.scatterplot(data=baysor_seg_downsampled_stmn1, x='x', y='y', hue='ncv_stmn1', s=9, ax=axes[3], palette='Reds')\n",
    "ax.legend().remove()\n",
    "ax = sns.scatterplot(data=baysor_seg_downsampled_anxa1, x='x', y='y', hue='ncv_anxa1', s=9, ax=axes[3], palette='Blues')\n",
    "ax.legend().remove()\n",
    "axes[3].set_xlim(200, 500)\n",
    "axes[3].set_ylim(300, 600)  # Invert y-axis for the scatter plot\n",
    "axes[3].set_title('Gene expression distribution (zoomed-in)')\n",
    "\n",
    "# Create custom legend\n",
    "green_patch = mpatches.Patch(color='green', label='PTPRC')\n",
    "red_patch = mpatches.Patch(color='red', label='STMN1')\n",
    "blue_patch = mpatches.Patch(color='blue', label='ANXA1')\n",
    "\n",
    "plt.legend(handles=[green_patch, red_patch, blue_patch], title='Gene expression', loc='upper right', bbox_to_anchor=(1.5, 1))\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUDING DISCUSSION\n",
    "1. What are the benefits of segmentation-free analysis?\n",
    "2. Can we replace segmentation with these?\n",
    "3. What are some of the real world use case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> END OF MODULE 4\n",
    ">>> SEE YOU NEXT for MODULE 5 & 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xenium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
