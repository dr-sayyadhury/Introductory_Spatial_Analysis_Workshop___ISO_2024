{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> **ISO2024 INTRODUCTORY SPATIAL 'OMICS ANALYSIS**\n",
    ">\n",
    ">\n",
    ">- HYBRID : TORONTO & ZOOM\n",
    ">- 9TH JULY 2024 <br>\n",
    "\n",
    ">**Module 3 : Building your spatial model** ** PART I ** <BR>\n",
    ">\n",
    ">**Instructor : Shamini Ayyadhury**\n",
    ">\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TOPICS COVERED\n",
    "\n",
    "* A. Normalizing your data *\n",
    "* B. Spatial and non-spatial clustering *\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\n",
    "A. NORMALIZATION AND DIMENSIONAL REDUCTION\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> IMPORT PACKAGES AND MANAGE PATHS TO YOUR FOLDERS AND FILES AS USUAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import the following libraries. Some of these were used in the previous notebook and we are using them here as well\n",
    "import sys # system specific parameters and functions\n",
    "import os # operating system dependent functionality\n",
    "\n",
    "import pandas as pd # data manipulation and analysis\n",
    "\n",
    "import matplotlib.pyplot as plt # plotting library\n",
    "import seaborn as sns # data visualization library based on matplotlib\n",
    "\n",
    "import scanpy as sc # single-cell analysis in Python\n",
    "\n",
    "sys.path.append('/home/shamini/data/projects/spatial_workshop/')\n",
    "import pre_processing_fnc as ppf # import memory usage function only\n",
    "\n",
    "\n",
    "#----------------------------------------------\n",
    "ppf.get_memory_usage() ### monitor memory usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### directory & filepaths\n",
    "\n",
    "data_dir = '/home/shamini/data1/data_orig/data/spatial/xenium/10xGenomics/'\n",
    "out = '/home/shamini/data/projects/spatial_workshop/out/'\n",
    "\n",
    "os.makedirs(out+'module3/TgCRND8_17_8mths', exist_ok=True) # create a new directory to store the output files\n",
    "\n",
    "\n",
    "#----------------------------------------------\n",
    "ppf.get_memory_usage() ### monitor memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> IMPORT ANNADATA FROM MODULE 2\n",
    "\n",
    "1. Note that we will using the same anndata object that we had created for the TgCRND8 sample.\n",
    "2. A supplementary script will process the wildtype sample and you will find this already saved in your module folder.\n",
    "\n",
    "3. Prior to this module, cell_type labels from the allen dataset have been transferred over to the anndata objects of the samples you will use in this module.\n",
    "\n",
    "4. You will now transfer the labels directly into the adata.obs slot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### IMPORT DATA\n",
    "\n",
    "'''\n",
    "1. ANNDATA OBJECT FROM MODULE 2\n",
    "2. A PREVIOUSLY ANNOTATED CELL LABEL FOR THE MICE BRAIN - DERIVED FROM THE ALLEND BRAIN ATLAS\n",
    "    *** students can look at supplementary script 01 to see how the cell label was processed\n",
    "'''\n",
    "\n",
    "### load previously processed AD mouse sample anndata object that we created from module 2\n",
    "adata = sc.read_h5ad(out+'module2/TgCRND8_17_8mths/adata.h5ad')\n",
    "print(adata)\n",
    "\n",
    "### load the cell label for the AD mouse sample that was already created for you - see supplementary script 01\n",
    "cell_label = pd.read_csv(out+'module3/allen_annotations/TgCRND8_allen.csv', index_col=0)\n",
    "cell_label = cell_label['predicted.id']\n",
    "print(cell_label)\n",
    "\n",
    "\n",
    "#----------------------------------------------\n",
    "ppf.get_memory_usage() ### monitor memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ATTACH CELL LABELS TO THE ANNDATA OBJECT, BY CONCATENATING WITH THE OBSERVATION DATAFRAME\n",
    "The number of cells labelled was missing slightly under 1000 cells, so we will drop the cells and their corresponding data from the anndata object\n",
    "'''\n",
    "\n",
    "# Subset adata to include only the cells present in cell_label\n",
    "common_indices = adata.obs.index.intersection(cell_label.index)\n",
    "adata = adata[common_indices].copy()\n",
    "\n",
    "# Reindex cell_label to match adata_subset\n",
    "cell_label_subset = cell_label.reindex(adata.obs.index)\n",
    "adata.obs['cell_label'] = cell_label_subset\n",
    "\n",
    "adata = adata[~adata.obs['cell_label'].isna()]\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(\"\\nSubset AnnData object created successfully!\")\n",
    "print(adata)\n",
    "\n",
    "\n",
    "#----------------------------------------------\n",
    "ppf.get_memory_usage() ### monitor memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> STOP TO EXPLAIN THE CODE IN THE NEXT BLOCK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NORMALIZATION & DIMENSIONAL REDUCTION - BASIC STEPS \n",
    "1. We will will go through the primary steps of normalization - to keep the flow of the methods intact.\n",
    "2. We will using 2 normalization methods \n",
    "    * Standard size factor normalization & lop1p transformation\n",
    "    * Pearson residuals \n",
    "3. We will have a short discussion on the need for developmental in normalization and dimensional reduction methods before moving onto clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "adata.X = adata.raw.X.copy()\n",
    "\n",
    "# 1. Pearson residual normalization\n",
    "### perform the pearson residual transformation\n",
    "adata.X = adata.raw.X.copy() ### reset the adata.X to the raw data\n",
    "sc.experimental.pp.normalize_pearson_residuals(adata) ### perform the pearson residual transformation\n",
    "adata.layers['pearson_residual'] = adata.X.copy() ### save the normalized and transformed adata.X in the layer\n",
    "\n",
    "sc.pp.pca(adata, n_comps=9, svd_solver='arpack') ### perform PCA\n",
    "adata.obsm['pearsonPCA'] = adata.obsm['X_pca'].copy() ### save the X_PCA coordinates to another layer labeled pearsonPCA\n",
    "\n",
    "sc.pp.neighbors(adata, n_pcs=9, n_neighbors=15) ### find the neighbors\n",
    "adata.uns['pearson_neigh'] = adata.uns['neighbors'] ### save the neighbors in another layer\n",
    "\n",
    "sc.tl.umap(adata, neighbors_key='pearson_neigh', min_dist=0.1, spread=1.0, method='umap', init_pos='spectral') ### perform UMAP\n",
    "adata.obsm['pearsonUMAP'] = adata.obsm['X_umap'].copy() ### save the UMAP coordinates to another layer labeled pearsonUMAP\n",
    "\n",
    "\n",
    "# 2. Standard normalization\n",
    "### now perform the standard normalization and transformation\n",
    "adata.X = adata.raw.X.copy() ### reset the adata.X to the raw data\n",
    "sc.pp.normalize_total(adata, target_sum=1e4) ### set the size factor to 1e4\n",
    "\n",
    "sc.pp.log1p(adata) ### perform the log1p transformation\n",
    "adata.layers['log1p_scaled'] = adata.X.copy() ### save the normalized and transformed adata.X in the layer\n",
    "\n",
    "sc.pp.pca(adata, n_comps=9, svd_solver='arpack') ### perform PCA\n",
    "adata.obsm['stdPCA'] = adata.obsm['X_pca'].copy() ### save the X_PCA coordinates to another layer labeled stdPCA\n",
    "\n",
    "sc.pp.neighbors(adata, n_pcs=9, n_neighbors=15) ### find the neighbors\n",
    "adata.uns['std_neigh'] = adata.uns['neighbors'] ### save the neighbors in another layer\n",
    "\n",
    "sc.tl.umap(adata, neighbors_key='std_neigh', min_dist=0.1, spread=1.0, method='umap', init_pos='spectral') ### perform UMAP\n",
    "adata.obsm['stdUMAP'] = adata.obsm['X_umap'].copy() ### save the UMAP coordinates to another layer labeled stdUMAP\n",
    "\n",
    "#sc.tl.leiden(adata, resolution=0.3, key_added='std_leiden_0.3')\n",
    "\n",
    "\n",
    "#----------------------------------------------\n",
    "ppf.get_memory_usage() ### monitor memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "fig.suptitle('A1. log1p vs Pearson residual normalization/tranformation', fontsize=18, y=1.05, x=0.3)\n",
    "\n",
    "sc.pl.embedding(adata, basis='stdUMAP', \n",
    "                color='cell_label', \n",
    "                title='Standard Transformation', \n",
    "                legend_loc='on data', \n",
    "                ax=axes[0], \n",
    "                show=False)\n",
    "\n",
    "sc.pl.embedding(adata, \n",
    "                basis='pearsonUMAP', \n",
    "                color='cell_label', \n",
    "                title='Pearson normalization', \n",
    "                legend_loc='on data', \n",
    "                ax=axes[1], \n",
    "                show=False)\n",
    "\n",
    "\n",
    "#----------------------------------------------\n",
    "ppf.get_memory_usage() ### monitor memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    ">>> NOW YOU WILL EXPERIMENT WITH TWO ADDITIONAL METHODS OF NORMALIZATION/TRANSFORMATION\n",
    "\n",
    "1. FIRST : YOU WILL REPEAT THE LOG1P TRANSFORMATION BUT BY CHANGING THE SIZE FACTOR USED TO NORMALIZE THE DATA.\n",
    "2. SECOND : YOU WILL TRY A NEW METHOD CALLED THE FREEMAN-TUKEY TRANSFORMATION\n",
    "\n",
    "3. THEN WE WILL REVIEW ALL 4 NORMALIZATION & TRANSFORMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Median size factor normalization\n",
    "### now perform the standard normalization and transformation\n",
    "\n",
    "adata.X = adata.raw.X.copy() ### reset the adata.X to the raw data\n",
    "median = np.median(adata.X.sum(axis=1)) ### calculate the median of the sum of the adata.X\n",
    "\n",
    "scale_factors = median / adata.X.sum(axis=1) ### calculate the scale factors\n",
    "adata.X = adata.X * (scale_factors[:, np.newaxis])  ### multiply the adata.X by the scale factors\n",
    "\n",
    "sc.pp.log1p(adata) ### perform the log1p transformation\n",
    "adata.layers['log1p_med'] = adata.X.copy() ### save the normalized and transformed adata.X in the layer\n",
    "\n",
    "sc.pp.pca(adata, n_comps=9, svd_solver='arpack') ### perform PCA\n",
    "adata.obsm['medPCA'] = adata.obsm['X_pca'].copy() ### save the X_PCA coordinates to another layer labeled stdPCA\n",
    "\n",
    "sc.pp.neighbors(adata, n_pcs=9, n_neighbors=27) ### find the neighbors\n",
    "adata.uns['med_neigh'] = adata.uns['neighbors'] ### save the neighbors in another layer\n",
    "\n",
    "sc.tl.umap(adata, neighbors_key='med_neigh', min_dist=0.1, spread=1.0, method='umap', init_pos='spectral') ### perform UMAP\n",
    "adata.obsm['medUMAP'] = adata.obsm['X_umap'].copy() ### save the UMAP coordinates to another layer labeled stdUMAP\n",
    "\n",
    "#sc.tl.leiden(adata, resolution=0.3, key_added='std_leiden_0.3')\n",
    "\n",
    "\n",
    "#----------------------------------------------\n",
    "ppf.get_memory_usage() ### monitor memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Freeman Tukey transformation \n",
    "adata.X = adata.raw.X.copy() ### reset the adata.X to the raw data\n",
    "\n",
    "### use numpy to perform the freeman tukey transformation\n",
    "adata.X = np.sqrt(adata.X) + np.sqrt(adata.X+1) ### perform the freeman tukey transformation\n",
    "adata.layers['freemanTukey'] = adata.X  ### save the normalized and transformed adata.X in the layer\n",
    "\n",
    "sc.pp.pca(adata, n_comps=9, svd_solver='arpack') ### perform PCA\n",
    "adata.obsm['freemanPCA'] = adata.obsm['X_pca'].copy() ### save the X_PCA coordinates to another layer labeled freemanPCA\n",
    "\n",
    "sc.pp.neighbors(adata, n_pcs=9, n_neighbors=27) ### find the neighbors\n",
    "adata.uns['freeman_neigh'] = adata.uns['neighbors'] ### save the neighbors in another layer\n",
    "sc.tl.umap(adata, neighbors_key='freeman_neigh', min_dist=0.1, spread=1.0, method='umap', init_pos='spectral') ### perform UMAP\n",
    "adata.obsm['freemanUMAP'] = adata.obsm['X_umap'].copy() ### save the UMAP coordinates to another layer labeled freemanUMAP\n",
    "\n",
    "\n",
    "#----------------------------------------------\n",
    "ppf.get_memory_usage() ### monitor memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "fig.suptitle('A2. Normalization and Transformation Methods', fontsize=18, y=1.05, x=0.3)\n",
    "sc.pl.embedding(adata, basis='pearsonUMAP', \n",
    "                color='cell_label', \n",
    "                title='Pearson Residual Normalization', \n",
    "                legend_loc='on data', \n",
    "                ax=axes[0], \n",
    "                show=False)\n",
    "\n",
    "sc.pl.embedding(adata, \n",
    "                basis='stdUMAP', \n",
    "                color='cell_label', \n",
    "                title='Standard Normalization', \n",
    "                legend_loc='on data', \n",
    "                ax=axes[1], \n",
    "                show=False)\n",
    "\n",
    "sc.pl.embedding(adata, \n",
    "                basis='medUMAP', \n",
    "                color='cell_label', \n",
    "                title='Median Normalization', \n",
    "                legend_loc='on data', \n",
    "                ax=axes[2], \n",
    "                show=False)\n",
    "\n",
    "sc.pl.embedding(adata, \n",
    "                basis='freemanUMAP', \n",
    "                color='cell_label', \n",
    "                title='Freeman Tukey Transformation', \n",
    "                legend_loc='on data', \n",
    "                ax=axes[3], \n",
    "                show=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------\n",
    "ppf.get_memory_usage() ### monitor memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    ">>> STOP FOR DISCUSSION/LECTURE\n",
    "\n",
    "*NORMALIZATION*\n",
    "> 1. ARE CURRENT METHODS FOR NORMALIZING SINGLE CELL DATASETS RELEVENT FOR SPATIAL DATA NORMALIZATION?\n",
    "> 2. WHAT ARE THE DIFFERENCES BETWEEN DROPLET/PLATE BASED SINGLE CELL SYSTEMS AND SINGLE MOLECULE IMAGING BASED SYSTEMS AND HOW DO YOU \n",
    "THINK THESE AFFECT THE COUNTS MODEL DISTRIBUTION AND SUBSEQUENT NORMALIATION METHODS?\n",
    "> 3. WHAT ASPECTS OF SPATIAL MODELING COULD INFLUENCE THE DEVELOPMENT OF NOVEL NORMALIZATION METHODS FOR SPATIAL TECHNOLOGIES?\n",
    "\n",
    "> *DIM REDUCTION METHODS*\n",
    "> 1. WE ARE USING THE SCANPY PACKAGE DERIVED UMAP, PCA , NEAREST NEIGHBOURHOOD CALCULATION FOR UMAP AND LEIDEN. ARE THE PARAMTERS STILL VALID FOR AND OPTIMAL FOR SINGLE MOLECULE IMAGING PLATFORMS LIKE THE XENIUM, MERSCOPE ?\n",
    "> 2. WHAT WOULD GIVE BETTER FLEXIBILITY IN EFFECTIVELY REDUCING DIMENSIONS AND IN IMPROVING EFFICIENCY?\n",
    ">    * PARTICIPANTS ARE ENCOURAGED TO TRY OUT THESE ALTERNATIVE PACKAGES WHICH GIVES MORE FLEXIBILITY IN PARAMTER TUNING AND TO USE MACHINE LEARNING MODELING TO SPEED PROCESSING AS SINGLE MOLECULE SPATIAL DATASETS ARE MANIFOLD LARGER THAN SINGLE CELL DATASETS\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> ADDITIONAL EXERCISE\n",
    "\n",
    "NOW YOU WILL COMPARE THE FREEMAN TUKEY TRANSFORMATION USING COUNTS DATA HAS BEEN SCALED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Freeman Tukey transformation AFTER scaling\n",
    "\n",
    "### use numpy to perform the freeman tukey transformation\n",
    "\n",
    "adata.X = adata.raw.X.copy() ### reset the adata.X to the raw data\n",
    "median = np.median(adata.X.sum(axis=1)) ### calculate the median of the sum of the adata.X\n",
    "\n",
    "scale_factors = median / adata.X.sum(axis=1) ### calculate the scale factors\n",
    "adata.X = adata.X * (scale_factors[:, np.newaxis])  ### multiply the adata.X by the scale factors\n",
    "\n",
    "adata.X = np.sqrt(adata.X) + np.sqrt(adata.X+1) ### perform the freeman tukey transformation\n",
    "adata.layers['freemanTukey_medN'] = adata.X  ### save the normalized and transformed adata.X in the layer\n",
    "\n",
    "sc.pp.pca(adata, n_comps=9, svd_solver='arpack') ### perform PCA\n",
    "adata.obsm['freeman_medN_PCA'] = adata.obsm['X_pca'].copy() ### save the X_PCA coordinates to another layer labeled freemanPCA\n",
    "\n",
    "sc.pp.neighbors(adata, n_pcs=9, n_neighbors=27) ### find the neighbors\n",
    "adata.uns['freeman_medN_neigh'] = adata.uns['neighbors'] ### save the neighbors in another layer\n",
    "sc.tl.umap(adata, neighbors_key='freeman_medN_neigh', min_dist=0.1, spread=1.0, method='umap', init_pos='spectral') ### perform UMAP\n",
    "adata.obsm['freeman_medN_UMAP'] = adata.obsm['X_umap'].copy() ### save the UMAP coordinates to another layer labeled freemanUMAP\n",
    "\n",
    "\n",
    "#----------------------------------------------\n",
    "ppf.get_memory_usage() ### monitor memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "fig.suptitle('A3. Freeman Tukey Transformation with and without scaling', fontsize=18, y=1.05, x=0.3)\n",
    "\n",
    "sc.pl.embedding(adata, \n",
    "                basis='freemanUMAP', \n",
    "                color='cell_label', \n",
    "                title='Freeman Tukey Transformation after median', \n",
    "                legend_loc='on data', \n",
    "                ax=axes[0], \n",
    "                show=False)\n",
    "\n",
    "sc.pl.embedding(adata, \n",
    "                basis='freeman_medN_UMAP', \n",
    "                color='cell_label', \n",
    "                title='med size factor normalization followed by Freeman Tukey Transformation', \n",
    "                legend_loc='on data', \n",
    "                ax=axes[1], \n",
    "                show=False)\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------\n",
    "ppf.get_memory_usage() ### monitor memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    ">>> STOP FOR DISCUSSION/LECTURE\n",
    "\n",
    "1. The point of the above exercise is NOT to show that normalization and transformation is important for visualization.\n",
    "2. The visualization tools we have used here - UMAP - is simply a medium to understand the concept.\n",
    "3. The main point here is - the choice of a normalization methods depends on many factors \n",
    "    * dataset & cell-types\n",
    "    * the plexity of your dataset\n",
    "    * platform - imaging versus sequencing \n",
    "4. Recall now your lessons from module 2\n",
    "    * Are the normalization methods we have used here appropriate?\n",
    "    * What additional considerations do we need to consider when normalizing image-based datasets?\n",
    "\n",
    "Remember - you are building a model <br>\n",
    "    * there are good models and worse models - but no wrong or right models <br>\n",
    "    * choose the model that gives you the best vantage point to answer your questions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adata.write_h5ad(out+'module3/TgCRND8_17_8mths/adata_module3a.h5ad') ### save the anndata object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    ">>> REVIEW FOR PART II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> END OF MODULE 3 : NORMALIZATION, DIMENSIONAL REDUCTION - PART 1 <br>\n",
    "> Thank you and see you in the next lecture where we will tackle spatial clustering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xenium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
